{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [Root]",
      "language": "python",
      "name": "Python [Root]"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "TwitterDataset.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYWPO0IV3qMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "import csv\n",
        "import theano.tensor as T\n",
        "import os.path\n",
        "from nltk.collocations import *\n",
        "from optparse import OptionParser\n",
        "from collections import Counter\n",
        "from copy import copy\n",
        "import cPickle\n",
        "import csv\n",
        "import warnings\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.cross_validation import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "from keras.layers.convolutional import MaxPooling1D, Convolution1D\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential, Graph\n",
        "from keras.engine.training import slice_X\n",
        "from keras.layers.core import Layer, Dense, Dropout, Activation,\\\n",
        "    Reshape, Flatten, Lambda\n",
        "from keras.regularizers import Regularizer\n",
        "from keras.optimizers import SGD\n",
        "from keras.constraints import maxnorm\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "\n",
        "from IPython.utils.io import CapturedIO\n",
        "from gensim.models import Word2Vec\n",
        "from pkg_resources import resource_filename\n",
        "import utils\n",
        "import datasets\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Yoon Kim's tokenization\n",
        "def my_process(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Every dataset is lower cased except for TREC\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^\\w(),|!?\\'\\`\\:\\-\\.;\\$%#]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" is\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" have\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" not\", string)\n",
        "    string = re.sub(r\"\\'re\", \" are\", string)\n",
        "    string = re.sub(r\"\\'d\", \" would\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" will\", string)\n",
        "    string = re.sub(r\"(?<=\\w)\\.\\.\\.\", \" ... \", string)\n",
        "    string = re.sub(r\"(?<=\\w)\\.\", \" . \", string)\n",
        "    string = re.sub(r\"(?<=\\w),\", \" , \", string)\n",
        "    string = re.sub(r\"(?<=\\w);\", \" ; \", string)\n",
        "    string = re.sub(r\"(?<=\\w)!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\((?=\\w)\", \" ( \", string)\n",
        "    string = re.sub(r\"(?<=\\w)\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"(?<=\\w)\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()\n",
        "\n",
        "def mixed_score(y_true, y_probs):\n",
        "    thresholds = sorted(unique(y_probs))\n",
        "    max_f1, best_threshold = 0, 0\n",
        "    for th in thresholds:\n",
        "        f1 = f1_score(y_true, y_probs >= th)*1)\n",
        "        if f1 > max_f1:\n",
        "            max_f1 = f1\n",
        "            best_th = th\n",
        "    th = best_th    \n",
        "    return {\n",
        "        \"precision\": y_true[y_probs >= th].sum()*1./(y_probs >= th).sum(),\n",
        "        \"recall\": y_true[y_probs >= th].sum()*1./y_true.sum(),\n",
        "        \"f1\": f1_score(y_true, (y_probs >= th)*1),\n",
        "        \"auc\": roc_auc_score(y_true, y_probs)\n",
        "    }\n",
        "\n",
        "# This function chooses the best threshold based on f1 of validation.\n",
        "def seq_score(model, X, y):\n",
        "    val_split = model.last_fit_params.get('validation_split', 0.)\n",
        "    split_at = int(model.last_fit_X.shape[0] * (1. - val_split))\n",
        "    X_val, y_val = model.last_fit_X[split_at:], model.last_fit_y[split_at:]\n",
        "    val_probs = model.predict(X_val).flatten()\n",
        "    thresholds = sorted(unique(val_probs))\n",
        "    max_f1, best_threshold = 0, 0\n",
        "    for threshold in thresholds:\n",
        "        f1 = f1_score(y_val, (val_probs >= threshold)*1)\n",
        "        if f1 > max_f1:\n",
        "            max_f1 = f1\n",
        "            best_threshold = threshold\n",
        "    return mixed_score(y, model.predict(X).flatten(), best_threshold)\n",
        "    \n",
        "# Same as seq_f1 but for graph model\n",
        "def graph_score(model, data):\n",
        "    val_split = model.last_fit_params.get('validation_split', 0.)\n",
        "    split_at = int(model.last_fit_data['output'].shape[0] * (1. - val_split))\n",
        "    data_val = {k: slice_X(v, split_at) for k, v in model.last_fit_data.items()}\n",
        "    val_probs = model.predict(data_val)['output'].flatten()\n",
        "    thresholds = sorted(unique(val_probs))\n",
        "    max_f1, best_threshold = 0, 0\n",
        "    for threshold in thresholds:\n",
        "        f1 = f1_score(data_val['output'], (val_probs >= threshold)*1)\n",
        "        if f1 > max_f1:\n",
        "            max_f1 = f1\n",
        "            best_threshold = threshold\n",
        "    return mixed_score(data['output'], \n",
        "                       model.predict(data)['output'].flatten(), \n",
        "                       best_threshold)\n",
        "\n",
        "def seq_auc(model, X, y):\n",
        "    preds = model.predict(X).flatten()\n",
        "    return roc_auc_score(y, preds)\n",
        "    \n",
        "def graph_auc(model, data):\n",
        "    preds = model.predict(data)['output'].flatten()\n",
        "    return roc_auc_score(data['output'], preds)\n",
        "\n",
        "seq_eval_f = seq_score\n",
        "graph_eval_f = graph_score\n",
        "results = pandas.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MQENpyN3qMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mixed_score(y_true, y_probs, th):  \n",
        "    y_true, y_probs = asarray(y_true), asarray(y_probs)\n",
        "    return {\n",
        "        \"precision\": y_true[y_probs >= th].sum()*1./(y_probs >= th).sum(),\n",
        "        \"recall\": y_true[y_probs >= th].sum()*1./y_true.sum(),\n",
        "        \"f1\": f1_score(y_true, (y_probs >= th)*1),\n",
        "        \"auc\": roc_auc_score(y_true, y_probs)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojY-9ahJ3qMx",
        "colab_type": "text"
      },
      "source": [
        "# Load ADR Twitter data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG07NEuv3qMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"/home/trung/data/psb2016/\"\n",
        "tweets, clean_tweets, labels = [], [], []\n",
        "with open(os.path.join(data_path, 'tweets.txt')) as f:\n",
        "    for line in f:\n",
        "        user_id, tweet_id, label, tweet = line.strip().split('\\t')\n",
        "        tweets.append(unidecode(tweet.decode('utf-8')))\n",
        "        labels.append(int(label))\n",
        "    \n",
        "np.random.seed(0)\n",
        "# Shuffle the data as Keras won't shuffle validation data.\n",
        "# This can make the training ends early as we are using\n",
        "# early stop for regularisation.\n",
        "idx = np.random.permutation(len(tweets))\n",
        "tweets, labels = asarray(tweets)[idx], asarray(labels)[idx]\n",
        "skf = list(StratifiedKFold(labels, n_folds=10))\n",
        "texts = asarray(tweets, dtype='str')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtzCvTie3qM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NU4bGFq3qM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxHwYaf23qM4",
        "colab_type": "text"
      },
      "source": [
        "# The baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__CkNzqx3qM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from zhang_adr.concept_matching import run_cm\n",
        "from zhang_adr.maxent_tfidf import run_tfidf\n",
        "from zhang_adr.maxent_nblcr import run_nblcr\n",
        "from zhang_adr.maxent_we import run_we\n",
        "from zhang_adr.tweetnlp import tweet_tagger\n",
        "from zhang_adr.preprocess import clean_tweet\n",
        "\n",
        "tokens, tags = tweet_tagger.runtagger_parse(texts)\n",
        "zhang_clean_texts = []\n",
        "for token, tag in zip(tokens, tags):\n",
        "    zhang_clean_texts.append(clean_tweet(token, tag))\n",
        "zhang_clean_texts = asarray(zhang_clean_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDtGlUcP3qM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "results = results[results['model'] != 'CM']\n",
        "results = results[results['model'] != 'ME-TFIDF']\n",
        "results = results[results['model'] != 'ME-NBLCR']\n",
        "results = results[results['model'] != 'ME-WE']\n",
        "clf = LogisticRegression(class_weight=\"auto\")\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    for i, (train_idx, test_idx) in enumerate(skf, 1):\n",
        "        print \"### Fold {}:\".format(i)\n",
        "        train, test = [], []\n",
        "        for train_id in train_idx:\n",
        "            train.append({\"id\": None, \"label\": labels[train_id], \"text\": zhang_clean_texts[train_id]})\n",
        "        train = pandas.DataFrame(train)\n",
        "        for test_id in test_idx:\n",
        "            test.append({\"id\": None, \"label\": labels[test_id], \"text\": zhang_clean_texts[test_id]})\n",
        "        test = pandas.DataFrame(test)\n",
        "\n",
        "        result = {}\n",
        "\n",
        "        y_pred_cm = run_cm(train, test, resource_filename('zhang_adr', 'data/ADR-lexicon.txt'))\n",
        "        result = mixed_score(test['label'].values, y_pred_cm, 0.5)\n",
        "        result['model'] = 'CM'\n",
        "        results = pandas.concat([results, pandas.DataFrame([result])])\n",
        "\n",
        "        _, y_prob_tfidf = run_tfidf(train, test, grams='123', n_dim=40000, clf=clf)\n",
        "        result = mixed_score(test['label'].values, asarray(y_prob_tfidf[:, 1]), 0.5)\n",
        "        result['model'] = 'ME-TFIDF'\n",
        "        results = pandas.concat([results, pandas.DataFrame([result])])\n",
        "\n",
        "        _, y_prob_nblcr = run_nblcr(train, test, 'nblcr', grams='123', clf=clf)\n",
        "        result = mixed_score(test['label'].values, y_prob_nblcr[:, 1], 0.5)\n",
        "        result['model'] = 'ME-NBLCR'\n",
        "        results = pandas.concat([results, pandas.DataFrame([result])])\n",
        "\n",
        "        _, y_prob_we = run_we(train, test, resource_filename('zhang_adr', 'data/w2v_150.txt'), 150, clf=clf)\n",
        "        result = mixed_score(test['label'].values,  y_prob_we[:, 1], 0.5)\n",
        "        result['model'] = 'ME-WE'\n",
        "        results = pandas.concat([results, pandas.DataFrame([result])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kskPJ2Ls3qM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.groupby('model').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAA5Qs8G3qM-",
        "colab_type": "text"
      },
      "source": [
        "# Our methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0MFlFGQ3qM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v = Word2Vec.load_word2vec_format(\n",
        "     '/home/trung/data/embeddings/word2vec_twitter_model/word2vec_twitter_model.bin', \n",
        "     binary=True\n",
        ")\n",
        "\n",
        "w2v = Word2Vec.load_word2vec_format(\n",
        "     '/home/trung/data/embeddings/GoogleNews.bin', \n",
        "     binary=True\n",
        " )\n",
        "\n",
        "w2v = Word2Vec.load_word2vec_format(\"/home/trung/data/embeddings/wlin/struc_skip_600.txt\")\n",
        "\n",
        "w2v = Word2Vec.load_word2vec_format(\n",
        "     resource_filename('zhang_adr', 'data/w2v_150.txt'),\n",
        "     binary=False,\n",
        " )\n",
        "\n",
        "w2v = Word2Vec.load_word2vec_format(\n",
        "    '/home/trung/data/embeddings/glovec/tmp',\n",
        "    binary=False\n",
        ")\n",
        "\n",
        "dim = w2v.layer1_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es61f9v_3qNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zhang_adr.TextUtility import TextUtility\n",
        "\n",
        "MOST_FREQUENT_WORDS = 20000\n",
        "USE_CACHE = False\n",
        "INCLUDE_UNKNOWN_WORDS = False\n",
        "\n",
        "docs = [[w for w in TextUtility.text_to_wordlist(text)\\\n",
        "         if INCLUDE_UNKNOWN_WORDS or w in w2v.index2word]\\\n",
        "         for text in zhang_clean_texts]\n",
        "all_words = Counter([w for doc in docs for w in doc])\n",
        "top_words = sorted(all_words.items(), key=lambda t: t[1], reverse=True)\n",
        "top_words = top_words[:MOST_FREQUENT_WORDS]\n",
        "V = {w:i for i, (w, freq) in enumerate(top_words)}\n",
        "X = utils.vectorize(docs, V)\n",
        "\n",
        "# initialize embedding matrix\n",
        "my_embeddings = np.random.normal(-.25, .25, size=(X.max() + 1, dim))\n",
        "for w in V:\n",
        "    if w in w2v:\n",
        "        my_embeddings[V[w]] = w2v[w]\n",
        "        \n",
        "# set embedding of padded character as 0s.\n",
        "my_embeddings[len(V) + 1] = np.zeros((dim, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUezn2nW3qNC",
        "colab_type": "text"
      },
      "source": [
        "### Logistic regression of sum of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A23lWSv3qND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def T_sum(x, **kwargs):\n",
        "    import theano.tensor as T\n",
        "    return T.sum(x, axis=-2)\n",
        "    \n",
        "def mk_lr_model_f(max_len, embeddings, embedding_fixed=False, \n",
        "                  optimizer='adadelta', loss='binary_crossentropy'):\n",
        "    \n",
        "    def lr_model():\n",
        "        m = Sequential()\n",
        "        m.add((utils.FixedEmbedding if embedding_fixed else Embedding)\\\n",
        "                (*embeddings.shape, input_length=max_len, weights=[embeddings]))\n",
        "        m.add(Lambda(T_sum, output_shape=(embeddings.shape[1],)))\n",
        "        m.add(Dense(1, activation='sigmoid'))\n",
        "        m.compile(loss=loss, optimizer=optimizer, class_mode='binary')\n",
        "        return m\n",
        "    \n",
        "    return lr_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeAW4VsA3qNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sequential model\n",
        "early_stopper = utils.MyEarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "scores = utils.seq_cross_validate(\n",
        "    mk_lr_model_f(X.shape[1], my_embeddings, optimizer='adadelta'),\n",
        "    X, labels, \n",
        "    skf, eval_f=seq_eval_f,\n",
        "    fit_params={\n",
        "        \"callbacks\": [early_stopper],\n",
        "        \"validation_split\": .1,\n",
        "        \"batch_size\": 50,\n",
        "        \"nb_epoch\": 100,\n",
        "    })\n",
        "results[\"my-lr-sum-embedding-dynamic-embedding\"] = scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mjtvhOf3qNI",
        "colab_type": "text"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaXiCTad3qNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # sequential model\n",
        "early_stopper = utils.MyEarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "model = utils.mk_yk_model_f(X.shape[1], my_embeddings, embedding_fixed=True, n_filters=300)()\n",
        "train_idx, test_idx = skf5[0]\n",
        "fit_params={\n",
        "     \"callbacks\": [early_stopper],\n",
        "     \"validation_split\": .1,\n",
        "     \"batch_size\": 50\n",
        " }\n",
        "model.fit(X[train_idx], labels[train_idx], **fit_params)\n",
        "\n",
        "idx = test_idx[(model.predict(X[test_idx]) >= 0.5).flatten()*1 != labels[test_idx]]\n",
        "wrongs = pandas.DataFrame(data={\"y\": labels[idx], \"docs\": asarray(docs)[idx], \n",
        "                                 \"zhang_clean_texts\": zhang_clean_texts[idx],\n",
        "                                 \"texts\": texts[idx]})\n",
        "wrongs.to_csv('incorrect.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOizFmGK3qNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sequential model\n",
        "early_stopper = utils.MyEarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
        "scores = utils.seq_cross_validate(\n",
        "    utils.mk_yk_model_f(X.shape[1], my_embeddings, n_filters=300),\n",
        "    X, labels, \n",
        "    skf, eval_f=seq_eval_f,\n",
        "    fit_params={\n",
        "        \"callbacks\": [early_stopper],\n",
        "        \"validation_split\": .1,\n",
        "        \"batch_size\": 50\n",
        "    }, \n",
        "    verbose=0)\n",
        "df = pandas.DataFrame(scores)\n",
        "model_name = \"CNN\"\n",
        "df[\"model\"] = model_name\n",
        "results = pandas.concat([results[results[\"model\"] != model_name], df])\n",
        "df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJUi-ehH3qNM",
        "colab_type": "text"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcw3Mf553qNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reload(utils)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q567kfAH3qNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sequential model\n",
        "early_stopper = utils.MyEarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
        "scores = utils.seq_cross_validate(\n",
        "    utils.mk_gru_model_f(X.shape[1], my_embeddings),\n",
        "    X[:, ::-1], labels,\n",
        "    skf, eval_f=seq_eval_f,\n",
        "    verbose=0,\n",
        "    fit_params={\n",
        "        \"callbacks\": [early_stopper],\n",
        "        \"validation_split\": .1,\n",
        "        \"batch_size\": 50\n",
        "    })\n",
        "df = pandas.DataFrame(scores)\n",
        "model_name = \"GRU\"\n",
        "df[\"model\"] = model_name\n",
        "results = pandas.concat([results[results[\"model\"] != model_name], df])\n",
        "df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwirK0ld3qNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.groupby('model').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TX-7-3F3qNd",
        "colab_type": "text"
      },
      "source": [
        "### CNN with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljszenx-3qNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopper = utils.MyEarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
        "scores = utils.graph_cross_validate(\n",
        "    utils.mk_attention_based_model_f(X.shape[1], my_embeddings, attention_l2=0.1),\n",
        "    {\"tokens\": X, \"output\": labels},\n",
        "    skf,\n",
        "    eval_f=graph_eval_f,\n",
        "    verbose=0,\n",
        "    fit_params={\n",
        "        \"callbacks\": [early_stopper],\n",
        "        \"validation_split\": .1,\n",
        "        \"batch_size\": 50\n",
        "    })\n",
        "df = pandas.DataFrame(scores)\n",
        "model_name = \"CNNA\"\n",
        "df[\"model\"] = model_name\n",
        "results = pandas.concat([results[results[\"model\"] != model_name], df])\n",
        "df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cbeQLzYd3qNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = utils.mk_attention_based_model_f(X.shape[1], my_embeddings)()\n",
        "early_stopper = utils.MyEarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "m.fit({\"tokens\": X[skf[0][0]], \"output\": labels[skf[0][0]]},\n",
        "      callbacks=[early_stopper],\n",
        "      batch_size=50,\n",
        "      validation_split=.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ671lBP3qNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "attentions = K.function([m.nodes[\"embedding\"].get_input()],\n",
        "                         [m.nodes[\"norm_attn_weights\"].get_output()])\n",
        "\n",
        "features = K.function([m.nodes[\"embedding\"].get_input()],\n",
        "                         [m.nodes[\"features\"].get_output()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqWGY0T3qNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"had\" in w2v.index2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "fowLfSTi3qNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in xrange(1000):\n",
        "    if len(docs[skf[0][0][i]]) >= 8 and labels[skf[0][0][i]]==1:\n",
        "        print tweets[skf[0][0][i]]\n",
        "        print zhang_clean_texts[skf[0][0][i]]        \n",
        "        print zip(docs[skf[0][0][i]], attentions([X[skf[0][0][i:i+1]]])[0].flatten())\n",
        "        print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "douhmFid3qNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "asarray(docs[skf[0][0][1:2]])\n",
        "attentions([X[skf[0][0][1:2]]])[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}